# Chubby Bird: A Custom Reinforcement Learning Environment with a Trained DQN Agent

A production-grade reinforcement learning project showcasing Deep Q-Network (DQN) training in a custom 2D game environment. This repository demonstrates the complete ML pipeline: problem formulation, state/reward design, agent training, and competitive gameplay.

## ğŸ® Project Demo

[![Chubby Bird AI Gameplay](https://img.youtube.com/vi/2tcEPlWI3Kg/0.jpg)](https://youtu.be/2tcEPlWI3Kg "Click to play on YouTube")

Click the image to watch the trained DQN agent in action!

This video demonstrates the final trained agent interacting with the environment in real time.  
You can observe decision making, food collection behavior, gravity handling, and failure cases learned through reinforcement learning.

## ğŸ“Š Project Overview

**Chubby Bird** is not just a gameâ€”it's a reinforcement learning environment where an AI agent learns optimal decision-making through self-play. The agent learns to navigate a dynamic 2D space, collect time-sensitive targets, and maximize survival time.

### Problem Statement
- **Environment**: A 2D scrolling game with physics-based bird dynamics
- **Objective**: Train an AI agent to catch falling food objects while avoiding ground collision
- **Challenge**: Reward shaping and exploration-exploitation balance in a continuous action environment

### Key Results
- âœ… Agent learns to actively seek food (vs. passive survival strategies)
- âœ… Achieves consistent food collection rates after 50 episodes
- âœ… Beats untrained baseline in competitive gameplay
- âœ… Trains in ~10 minutes on CPU

---

## ğŸ§  The Reinforcement Learning Problem

### State Space (Observation)
The agent observes 4-dimensional state vector at each timestep:

```
State = [bird_y, bird_velocity, food_dx, food_dy]

â€¢ bird_y       : Bird's vertical position (normalized 0-1, 0=top, 1=bottom)
â€¢ bird_velocity: Current vertical velocity (normalized, range -1 to 1.5)
â€¢ food_dx      : Horizontal distance to nearest food (normalized -1 to 1)
â€¢ food_dy      : Vertical distance to nearest food (normalized -1 to 1)
```

**Design Rationale**: This minimal 4D representation captures the essential control problemâ€”vertical positioning and proximity awarenessâ€”without computational overhead.

### Action Space (Control)
The agent has 2 discrete actions:

```
action = 0: Do nothing (gravity pulls bird down)
action = 1: Flap wings (apply upward impulse)
```

**Design Rationale**: Simple binary control mimics Flappy Bird constraints while remaining Markovian and deterministic.

### Reward Design (Engineering Focus)
The reward function was carefully engineered to avoid local optima and reward hacking:

| Trigger | Reward | Purpose |
|---------|--------|---------|
| Collect food | +10.0 | Primary objective |
| Miss food (escape) | -2.0 | Penalize inaction |
| In safe middle region | +0.1 | Exploration incentive |
| At top of screen | -2.0 | Force downward diversity |
| Near ground | -0.5 | Discourage reckless play |
| Within 100px of food | +0.05 | Proximity guidance |
| Per step | -0.005 | Efficiency penalty |

**Critical Engineering Decisions:**

1. **No inflated proximity bonus** - Removed +0.5/step bonus (agent hovered near food without catching it)
2. **Only reward actual catches** - Not generic "moving toward" (eliminated false positives)
3. **Random spawn positions** - Prevents overfitting to starting in middle
4. **Score-based model saving** - Save on food caught, not total reward (prevents reward gaming)

---

## ğŸ¤– Agent Architecture

### DQN Implementation
```
Deep Q-Network (DQN) Architecture:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Input Layer:     4 neurons (state dimensions)
Hidden Layer 1:  128 neurons + ReLU activation
Hidden Layer 2:  128 neurons + ReLU activation
Output Layer:    2 neurons (Q-values per action)
```

### Training Configuration
```python
Episodes:               50
Max steps/episode:      3000
Learning rate:          0.001 (Adam optimizer)
Discount factor (Î³):    0.99
Epsilon decay:          0.98 per episode (slower exploration decay)
Epsilon min:            0.10 (maintain 10% exploration)
Batch size:             64
Memory buffer:          5000 experiences
Target update freq:     200 steps
```

### Stabilization Techniques
1. **Target Network Freezing**: Separate frozen target network updated every 200 steps (prevents feedback loops)
2. **Gradient Clipping**: `clip_grad_norm_(max_norm=1.0)` (prevents exploding gradients)
3. **Experience Replay**: Mini-batch SGD from randomized memory buffer (breaks temporal correlations)

---

## ğŸ® System Architecture Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      GAME LOOP (60 FPS)                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Get Game State  â”‚
                    â”‚  [y,v,dx,dy]     â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚  DQN Agent (Inference Mode)   â”‚
              â”‚  â€¢ Forward pass through net   â”‚
              â”‚  â€¢ Q(s,a) = [Q_nothing,Q_flap]â”‚
              â”‚  â€¢ action = argmax(Q-values)  â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Execute Action  â”‚
                    â”‚  Physics update  â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  Observe: Reward + Next State       â”‚
        â”‚  â€¢ Food collision? +10              â”‚
        â”‚  â€¢ Food escaped? -2                 â”‚
        â”‚  â€¢ New state: [y',v',dx',dy']       â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Store Transition in Memory          â”‚
        â”‚ (state, action, reward, next_state) â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  Train on Mini-Batch (when ready)   â”‚
        â”‚  â€¢ Sample 64 transitions            â”‚
        â”‚  â€¢ Forward pass on current net      â”‚
        â”‚  â€¢ Compute target Q with frozen net â”‚
        â”‚  â€¢ MSE loss + backprop              â”‚
        â”‚  â€¢ Gradient clip + Adam step        â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ˆ Training Results

### Learning Performance

**Total Reward per Episode:**
![Reward vs Episode](results/reward_vs_episode.png)

The agent shows clear learning progression. Early episodes incur heavy exploration penalties (-5000 range), stabilizing as the agent learns efficient strategies.

**Food Collection Rate:**
![Score vs Episode](results/score_vs_episode.png)

Agent learns to catch 3-5 food items by episode 10 and maintains consistent performance by episode 25, demonstrating stable convergence.

---

## ğŸ¯ Engineering Challenges & Solutions

| Challenge | Root Cause | Fix | Result |
|-----------|-----------|-----|--------|
| **Reward Hacking** | +0.5 proximity bonus caused hovering | Removed proximity bonus, reward only catching | Agent actively catches food |
| **Stuck in Local Optima** | Îµ decay 0.999/ep â†’ 0.01 in 7 eps | Changed to 0.98/ep, maintains 0.10 floor | Agent explores all regions |
| **Training Instability** | No gradient clipping, freq updates | Added clip_grad_norm(1.0), 200-step targets | Stable convergence |
| **Poor Generalization** | Always spawn at middle (y=0.5) | Random spawn [50px, height-100px] | Robust at any position |
| **Distribution Shift** | Model assumes middle-start position | Randomize bird Y during training | Handles diverse initial states |

### Key Engineering Decisions Explained

**Reward Shaping Pitfall**: Initial +0.5/step bonus for being "close to food" caused reward hackingâ€”agent learned to hover near targets for points without actually catching them. Solution: Only reward terminal actions (food catch +10, food escape -2).

**Exploration Decay**: Epsilon 0.999/step converged too fast, locking into "stay at top" strategy. Slower decay (0.98/episode) with 0.10 floor maintained 10% exploration throughout training, enabling discovery of diverse strategies.

**Gradient Stability**: No clipping caused exploding gradients during food-rich episodes. Clip_grad_norm(max_norm=1.0) prevented divergence while maintaining learning speed.

---

## ğŸš€ Usage

### Quick Start
```bash
# 1. Install dependencies
pip install -r requirements.txt

# 2. Play
python main.py
```

### Training Your Own Model
```bash
python train.py
# Trains for 50 episodes, saves assets/model/best_model.pth
# Shows live rendering during training (~15 min)
# Edit train.py config for faster headless training
```
---

## ğŸ® Game Modes

### 1. Manual Play
- Control with **SPACEBAR** to flap
- Collect falling food for points
- Game ends on ground collision
- **Purpose**: Understand game mechanics

### 2. AI Play  
- Watch the trained agent play
- No user input
- Shows AI's decision-making in action
- **Purpose**: Verify training effectiveness

### 3. You vs AI
- Alternating turns: Player then AI
- Each player tries to catch as much food as possible
- First to 10 points wins
- **Purpose**: Competitive benchmark

---

## ğŸ“ Project Structure

```
Chubby Bird/
â”œâ”€â”€ main.py                    # Entry point
â”œâ”€â”€ launcher.py                # Game mode router
â”œâ”€â”€ train.py                   # DQN training script
â”œâ”€â”€ requirements.txt           # Dependencies
â”œâ”€â”€ assets/model/best_model.pth             # Trained agent weights
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ settings.py            # Game constants
â”‚   â”œâ”€â”€ agent.py               # DQN model + training logic
â”‚   â”œâ”€â”€ env.py                 # Training environment
â”‚   â”œâ”€â”€ game.py                # Base game loop
â”‚   â”œâ”€â”€ vs_game.py             # Competitive mode
â”‚   â”œâ”€â”€ menu_simple.py         # Menu UI
â”‚   â”œâ”€â”€ bird.py                # Physics + rendering
â”‚   â””â”€â”€ food.py                # Food spawning
â”‚
â””â”€â”€ assets/
    â”œâ”€â”€ images/                # Sprites & backgrounds
    â””â”€â”€ sounds/                # Audio files
```

---

## ğŸ”§ Quick Reference

### Training Configuration
| Parameter | Value | Purpose |
|-----------|-------|---------|
| Episodes | 50 | Total training runs |
| Max steps/episode | 3000 | Timeout per episode |
| Learning rate | 0.001 | Adam optimizer |
| Discount factor (Î³) | 0.99 | Future reward weight |
| Epsilon decay | 0.98/ep | Exploration schedule |
| Epsilon min | 0.10 | Min exploration rate |
| Batch size | 64 | SGD mini-batch |
| Memory buffer | 5000 | Experience replay size |
| Target update | 200 steps | Frozen network sync |

### Game Modes
- **Manual Play**: Control with SPACEBAR
- **AI Play**: Watch trained agent
- **You vs AI**: Competitive mode (first to 10 wins)

---

## ğŸ“š What This Demonstrates

âœ… Deep Q-Learning (DQN)  
âœ… Experience replay & target networks  
âœ… Reward shaping in practice  
âœ… Hyperparameter tuning  
âœ… Agent evaluation metrics  
âœ… Competitive benchmarking  

---

## ğŸ‘¨â€ğŸ’» Author

**Mansoor Bukhari**
- GitHub: [@cyberfantics](https://github.com/cyberfantics)
- LinkedIn: [linkedin.com/in/mansoor-bukhari](https://linkedin.com/in/mansoor-bukhari)

---

## ğŸ“ License

MIT License - Use freely for learning and development.

---

## ğŸ™ Acknowledgments

- Inspired by Flappy Bird and DQN paper ([Human-level control through deep RL](https://www.nature.com/articles/nature14236))
- Thanks to Pygame community for excellent documentation
- PyTorch team for intuitive deep learning APIs


---

